{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "# import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Bidirectional, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "# import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# make sure to use position 1\n",
    "sys.path.insert(0, \"cdc/src/\")\n",
    "# import Attention\n",
    "#reload(Attention)\n",
    "from Attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(texts):\n",
    "    texts = [x.lower() for x in texts]\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    texts = [' '.join([word for word in x.split() if word not in set(stopwords.words(\"english\"))]) for x in texts]\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    return(texts)\n",
    "\n",
    "\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count = [['RARE', -1]]    \n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))    \n",
    "    word_dict = {}\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return(word_dict)\n",
    "\n",
    "\n",
    "def text_to_numbers(sentences, word_dict):\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        for word in sentence.split():\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)\n",
    "    \n",
    "    \n",
    "def build_embedding(embedding_file):\n",
    "    embeddings_index = {}\n",
    "    f = open(embedding_file)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_dictionary) + 1, embedding_size))\n",
    "    for i, word in word_dictionary_rev.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_layer = Embedding(top_words, embedding_size, input_length=max_words)\n",
    "    embedding_layer = Embedding(len(word_dictionary) + 1,\n",
    "                                embedding_size,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_words,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "    \n",
    "\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output])\n",
    "    activations = get_activations([X_batch, 0])\n",
    "    return activations\n",
    "\n",
    "\n",
    "def multiclass_roc_auc_score(y_true, y_pred, average='macro'):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_true)\n",
    "    y_true = lb.transform(y_true)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_true, y_pred, average=\"weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(f_path, l_path, wemb_file):\n",
    "    global vocabulary_size\n",
    "    global max_words\n",
    "    global top_words\n",
    "    global embedding_size\n",
    "    global word_dictionary\n",
    "    global word_dictionary_rev\n",
    "\n",
    "    vocabulary_size = 1000000\n",
    "    max_words = 300\n",
    "    top_words = 1000000\n",
    "    embedding_size = 300\n",
    "    attention = True\n",
    "\n",
    "    print(\"--- Loading data ---\")\n",
    "    texts = pd.read_csv(f_path, sep='\\t', encoding='latin-1')\n",
    "    texts = texts.ix[:, 1].values.tolist()\n",
    "    label = pd.read_csv(l_path, sep='\\t', header=None, encoding='latin-1')\n",
    "    label = label.values.tolist()\n",
    "    target = label\n",
    "    print(\"Read %d rows of data\" % len(texts))\n",
    "    print(\"Read %d rows of label\" % len(label))\n",
    "\n",
    "    # label encoder\n",
    "    print(\"--- Label encoding ---\")\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(target)\n",
    "    target = encoder.transform(target)\n",
    "    target = to_categorical(target)\n",
    "\n",
    "    # text preprocessing\n",
    "    print(\"--- Text processing ---\")\n",
    "    texts = normalize_text(texts)\n",
    "    word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "    word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "\n",
    "    text_data = text_to_numbers(texts, word_dictionary)\n",
    "    train_indices = np.random.choice(len(text_data), int(round(0.7 * len(text_data))), replace=False)\n",
    "    test_indices = np.array(list(set(range(len(text_data))) - set(train_indices)))\n",
    "    texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "    texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "    target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "    target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n",
    "    # Convert texts to lists of indices\n",
    "    text_data_train = np.array(text_to_numbers(texts_train, word_dictionary))\n",
    "    text_data_test = np.array(text_to_numbers(texts_test, word_dictionary))\n",
    "    # Pad/crop\n",
    "    text_data_train = np.array([x[0:max_words] for x in [y + [0] * max_words for y in text_data_train]])\n",
    "    text_data_test = np.array([x[0:max_words] for x in [y + [0] * max_words for y in text_data_test]])\n",
    "\n",
    "    # embedding\n",
    "    # https://fasttext.cc/docs/en/english-vectors.html\n",
    "    embedding_layer = build_embedding(wemb_file)\n",
    "\n",
    "    # model construction\n",
    "    print(\"--- Build model ---\")\n",
    "    sequence_input = Input(shape=(max_words,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    if attention == False:\n",
    "        m = Bidirectional(GRU(128))(embedded_sequences)\n",
    "    else:\n",
    "        m = Bidirectional(GRU(128, return_sequences=True, consume_less='mem'))(embedded_sequences)\n",
    "        m = Attention()(m)\n",
    "    pred = Dense(len(encoder.classes_), activation='softmax')(m)\n",
    "    model = Model(sequence_input, pred)\n",
    "    model.compile(loss=losses.categorical_crossentropy, optimizer=optimizers.Adam(),\n",
    "                  metrics=['accuracy', precision, recall, fmeasure])\n",
    "    print(model.summary())\n",
    "\n",
    "    # training\n",
    "    print(\"--- Training ---\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    model_fit = model.fit(text_data_train, target_train,\n",
    "                          validation_data=(text_data_test, target_test),\n",
    "                          epochs=100,\n",
    "                          batch_size=64,\n",
    "                          shuffle=True,\n",
    "                          callbacks=[early_stopping])\n",
    "    history = pd.DataFrame(model_fit.history)\n",
    "    history.to_csv(\"model_history.txt\", sep=\"\\t\")\n",
    "\n",
    "    # prediction\n",
    "    print(\"--- Prediction ---\")\n",
    "    scores = model.evaluate(text_data_test, target_test, verbose=0)\n",
    "    y_pred = model.predict(text_data_test)\n",
    "    auc = roc_auc_score(target_test, y_pred, average='weighted')\n",
    "    print(scores[1], scores[2], scores[3], scores[4], auc)\n",
    "\n",
    "    # save model  \n",
    "    print(\"--- Save model ---\")\n",
    "    json_model = model.to_json()\n",
    "    open('model.json', 'w').write(json_model)\n",
    "    model.save_weights('model_weights.h5', overwrite=True)\n",
    "    cPickle.dump(encoder, open('model_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_keras_backend(backend):\n",
    "        if K.backend() != backend:\n",
    "            os.environ['KERAS_BACKEND'] = backend\n",
    "            reload(K)\n",
    "            assert K.backend() == backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_keras_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data ---\n",
      "Read 431 rows of data\n",
      "Read 431 rows of label\n",
      "--- Label encoding ---\n",
      "--- Text processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999995 word vectors.\n",
      "--- Build model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(128, return_sequences=True, implementation=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 300)          3741300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 256)          329472    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 256)               556       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 4,072,870\n",
      "Trainable params: 331,570\n",
      "Non-trainable params: 3,741,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "--- Training ---\n",
      "Train on 302 samples, validate on 129 samples\n",
      "Epoch 1/100\n",
      "302/302 [==============================] - 4s 15ms/step - loss: 1.6714 - acc: 0.3377 - precision: 0.0571 - recall: 0.0099 - fmeasure: 0.0169 - val_loss: 1.6119 - val_acc: 0.3333 - val_precision: 0.2872 - val_recall: 0.0853 - val_fmeasure: 0.1315\n",
      "Epoch 2/100\n",
      "302/302 [==============================] - 3s 11ms/step - loss: 1.5187 - acc: 0.3974 - precision: 0.5375 - recall: 0.1093 - fmeasure: 0.1793 - val_loss: 1.5187 - val_acc: 0.3333 - val_precision: 0.3411 - val_recall: 0.0853 - val_fmeasure: 0.1364\n",
      "Epoch 3/100\n",
      "302/302 [==============================] - 3s 11ms/step - loss: 1.4643 - acc: 0.4172 - precision: 0.5988 - recall: 0.1060 - fmeasure: 0.1755 - val_loss: 1.5086 - val_acc: 0.3643 - val_precision: 0.3236 - val_recall: 0.1163 - val_fmeasure: 0.1711\n",
      "Epoch 4/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 1.4186 - acc: 0.4570 - precision: 0.5937 - recall: 0.1854 - fmeasure: 0.2804 - val_loss: 1.4751 - val_acc: 0.4341 - val_precision: 0.3435 - val_recall: 0.1395 - val_fmeasure: 0.1984\n",
      "Epoch 5/100\n",
      "302/302 [==============================] - 4s 12ms/step - loss: 1.3734 - acc: 0.5265 - precision: 0.6449 - recall: 0.1887 - fmeasure: 0.2900 - val_loss: 1.4384 - val_acc: 0.4574 - val_precision: 0.3841 - val_recall: 0.1860 - val_fmeasure: 0.2507\n",
      "Epoch 6/100\n",
      "302/302 [==============================] - 4s 13ms/step - loss: 1.3109 - acc: 0.5430 - precision: 0.6745 - recall: 0.2450 - fmeasure: 0.3586 - val_loss: 1.3336 - val_acc: 0.5581 - val_precision: 0.4564 - val_recall: 0.1783 - val_fmeasure: 0.2564\n",
      "Epoch 7/100\n",
      "128/302 [===========>..................] - ETA: 1s - loss: 1.2614 - acc: 0.5703 - precision: 0.8947 - recall: 0.1953 - fmeasure: 0.3159"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e052206374a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m train(data_home+'test/data/data.txt', \n\u001b[1;32m     11\u001b[0m      \u001b[0mdata_home\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test/data/label.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m      data_home+'test/data/wiki-news-300d-1M.vec')\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# embedding_layer = build_embedding(wemb_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-af572fef6874>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(f_path, l_path, wemb_file)\u001b[0m\n\u001b[1;32m     77\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                           callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_history.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#set_keras_backend(\"theano\")\n",
    "data_home = '/Users/zebinkang/Github/scalable_text_analysis/' # os.getcwd()\n",
    "wemb_file=data_home+'/test/data/crawl-300d-2M.vec'\n",
    "# embedding\n",
    "# https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "train(data_home+'test/data/data.txt', \n",
    "     data_home+'test/data/label.txt',\n",
    "     data_home+'test/data/wiki-news-300d-1M.vec')\n",
    "# embedding_layer = build_embedding(wemb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
